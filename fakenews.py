# -*- coding: utf-8 -*-
"""Fakenews1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LZKaKRl9iK7qGALhB7ixU-1BV26tVAaE
"""

# --- CELL 1: IMPORTS ---
import pandas as pd
import numpy as np
import re
import string
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, accuracy_score

# Machine Learning Models
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier, AdaBoostClassifier, StackingClassifier
from sklearn.svm import LinearSVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from xgboost import XGBClassifier

# Deep Learning Libraries
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Embedding, LSTM, GRU, SimpleRNN, Bidirectional, Conv1D, GlobalMaxPooling1D, Input, Dropout, LayerNormalization, MultiHeadAttention, Flatten
from tensorflow.keras.callbacks import EarlyStopping

print("All libraries imported successfully.")

# --- CELL 2: LOAD & CLEAN DATA ---
true = pd.read_csv('True.csv')
fake = pd.read_csv('Fake.csv')

true['label'] = 1
fake['label'] = 0

news = pd.concat([fake, true], axis=0)
news = news.drop(['title', 'subject', 'date'], axis=1)
news = news.sample(frac=1).reset_index(drop=True)

def wordopt(text):
    text = text.lower()
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\d', '', text)
    text = re.sub(r'\n', ' ', text)
    return text

news['text'] = news['text'].apply(wordopt)

x = news['text']
y = news['label']

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)
print("Data Split Complete.")

# --- CELL 3: VECTORIZATION & TOKENIZATION ---

# 1. TF-IDF for Machine Learning
vectorization = TfidfVectorizer()
xv_train = vectorization.fit_transform(x_train)
xv_test = vectorization.transform(x_test)
print("TF-IDF Vectorization Done.")

# 2. Tokenization for Deep Learning
vocab_size = 10000
max_length = 200
embedding_dim = 100
tokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>")
tokenizer.fit_on_texts(x_train)

train_sequences = tokenizer.texts_to_sequences(x_train)
test_sequences = tokenizer.texts_to_sequences(x_test)

train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')
test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')
print("DL Tokenization Done.")

# --- CELL 4: LOGISTIC REGRESSION ---
print("--- Logistic Regression ---")
LR = LogisticRegression(max_iter=1000)
LR.fit(xv_train, y_train)
pred_lr = LR.predict(xv_test)
print(classification_report(y_test, pred_lr))

# --- CELL 5: DECISION TREE ---
print("--- Decision Tree ---")
DTC = DecisionTreeClassifier()
DTC.fit(xv_train, y_train)
pred_dtc = DTC.predict(xv_test)
print(classification_report(y_test, pred_dtc))

# --- CELL 6: RANDOM FOREST ---
print("--- Random Forest ---")
RFC = RandomForestClassifier()
RFC.fit(xv_train, y_train)
pred_rfc = RFC.predict(xv_test)
print(classification_report(y_test, pred_rfc))

# --- CELL 7: GRADIENT BOOSTING ---
print("--- Gradient Boosting ---")
GBC = GradientBoostingClassifier()
GBC.fit(xv_train, y_train)
pred_gbc = GBC.predict(xv_test)
print(classification_report(y_test, pred_gbc))

# --- CELL 8: SVM, KNN, NAIVE BAYES ---
print("--- Linear SVM ---")
SVM = LinearSVC(max_iter=1000)
SVM.fit(xv_train, y_train)
pred_svm = SVM.predict(xv_test)
print(classification_report(y_test, pred_svm))

print("\n--- K-Nearest Neighbors (KNN) ---")
KNN = KNeighborsClassifier(n_neighbors=5)
KNN.fit(xv_train, y_train)
pred_knn = KNN.predict(xv_test)
print(classification_report(y_test, pred_knn))

print("\n--- Naive Bayes ---")
NB = MultinomialNB()
NB.fit(xv_train, y_train)
pred_nb = NB.predict(xv_test)
print(classification_report(y_test, pred_nb))

# --- CELL 9: ENSEMBLE METHODS ---
print("--- Bagging (Decision Tree) ---")
bagging_dt = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)
bagging_dt.fit(xv_train, y_train)
print(classification_report(y_test, bagging_dt.predict(xv_test)))

print("\n--- Bagging (Logistic Regression) ---")
bagging_lr = BaggingClassifier(estimator=LogisticRegression(), n_estimators=10, random_state=42)
bagging_lr.fit(xv_train, y_train)
print(classification_report(y_test, bagging_lr.predict(xv_test)))

print("\n--- AdaBoost ---")
ada = AdaBoostClassifier(n_estimators=50, random_state=42)
ada.fit(xv_train, y_train)
print(classification_report(y_test, ada.predict(xv_test)))

print("\n--- XGBoost ---")
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb.fit(xv_train, y_train)
print(classification_report(y_test, xgb.predict(xv_test)))

# --- CELL 10: STACKING CLASSIFIER ---
print("--- Stacking Classifier ---")
# Combining LR, RF, and SVM
estimators = [
    ('lr', LogisticRegression(max_iter=1000)),
    ('rf', RandomForestClassifier(n_estimators=50)),
    ('svm', LinearSVC(max_iter=1000))
]
stacking = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())
stacking.fit(xv_train, y_train)
print(classification_report(y_test, stacking.predict(xv_test)))

# --- CELL 11: DL SETUP ---
early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)

# Dictionary to store trained DL models for later use
dl_models = {}

def compile_fit(model, name):
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    print(f"\nTraining {name}...")
    model.fit(train_padded, y_train, epochs=5, batch_size=64, validation_split=0.2, callbacks=[early_stop], verbose=1)
    dl_models[name] = model
    # Evaluate
    loss, acc = model.evaluate(test_padded, y_test)
    print(f"{name} Accuracy: {acc:.4f}")

# --- CELL 12: RECURRENT NETWORKS ---

# 1. Simple RNN
print("--- 1. Simple RNN ---")
rnn_model = Sequential([
    Embedding(vocab_size, embedding_dim, input_length=max_length),
    SimpleRNN(128),
    Dense(1, activation='sigmoid')
])
compile_fit(rnn_model, "rnn")

# 2. GRU
print("\n--- 2. GRU ---")
gru_model = Sequential([
    Embedding(vocab_size, embedding_dim, input_length=max_length),
    GRU(128),
    Dense(1, activation='sigmoid')
])
compile_fit(gru_model, "gru")

# 3. LSTM
print("\n--- 3. LSTM ---")
lstm_model = Sequential([
    Embedding(vocab_size, embedding_dim, input_length=max_length),
    LSTM(128),
    Dense(1, activation='sigmoid')
])
compile_fit(lstm_model, "lstm")

# 4. Bi-Directional LSTM
print("\n--- 4. Bi-LSTM ---")
bilstm_model = Sequential([
    Embedding(vocab_size, embedding_dim, input_length=max_length),
    Bidirectional(LSTM(128)),
    Dense(1, activation='sigmoid')
])
compile_fit(bilstm_model, "bi-lstm")

# --- CELL 13: ADVANCED DL ARCHITECTURES ---

# 5. CNN (Convolutional Neural Network)
print("--- 5. CNN ---")
cnn_model = Sequential([
    Embedding(vocab_size, embedding_dim, input_length=max_length),
    Conv1D(128, 5, activation='relu'),
    GlobalMaxPooling1D(),
    Dense(1, activation='sigmoid')
])
compile_fit(cnn_model, "cnn")

# 6. Attention Model
print("\n--- 6. Attention ---")
inputs = Input(shape=(max_length,))
x = Embedding(vocab_size, embedding_dim)(inputs)
lstm_out = LSTM(128, return_sequences=True)(x)
att = Dense(1, activation='tanh')(lstm_out)
att = Flatten()(att)
att = Dense(max_length, activation='softmax')(att)
att = tf.keras.layers.RepeatVector(128)(att)
att = tf.keras.layers.Permute([2, 1])(att)
merged = tf.keras.layers.multiply([lstm_out, att])
merged = tf.keras.layers.Lambda(lambda x: tf.keras.backend.sum(x, axis=1))(merged)
outputs = Dense(1, activation='sigmoid')(merged)
att_model = Model(inputs=inputs, outputs=outputs)
compile_fit(att_model, "attention")

# 7. Transformer Model
print("\n--- 7. Transformer ---")
inputs = Input(shape=(max_length,))
x = Embedding(vocab_size, embedding_dim)(inputs)
att_out = MultiHeadAttention(num_heads=2, key_dim=embedding_dim)(x, x)
x = LayerNormalization(epsilon=1e-6)(x + att_out)
x = GlobalMaxPooling1D()(x)
x = Dropout(0.5)(x)
outputs = Dense(1, activation='sigmoid')(x)
trans_model = Model(inputs=inputs, outputs=outputs)
compile_fit(trans_model, "transformer")

# --- CELL 15: GENERATE FULL DETAILED EXCEL REPORT ---
import sys
import subprocess
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, accuracy_score, log_loss

# 1. Install xlsxwriter
try:
    import xlsxwriter
except ImportError:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "xlsxwriter"])
    import xlsxwriter

print("Generatng FULL report with Class 0/1 breakdown...")

results_list = []

def get_full_metrics(name, category, model, is_dl=False):
    # --- 1. GET PREDICTIONS ---
    if is_dl:
        # Deep Learning: Get Probabilities & Loss
        loss_val = model.evaluate(test_padded, y_test, verbose=0)[0]
        pred_prob = model.predict(test_padded, verbose=0)
        pred = (pred_prob > 0.5).astype(int).flatten()
    else:
        # Machine Learning: Get Predictions
        pred = model.predict(xv_test)

        # Try to get Loss (Log Loss) if supported
        try:
            if hasattr(model, "predict_proba"):
                pred_prob = model.predict_proba(xv_test)
                loss_val = log_loss(y_test, pred_prob)
            else:
                loss_val = "N/A"
        except:
            loss_val = "N/A"

    # --- 2. GENERATE FULL REPORT (The "Everything" part) ---
    # output_dict=True gives us the data structure to save in Excel
    report = classification_report(y_test, pred, output_dict=True)

    # --- 3. FLATTEN THE DATA FOR EXCEL ---
    # We map the dictionary keys to nice Excel Column names
    metrics = {
        "Category": category,
        "Model Name": name,
        "Accuracy": report['accuracy'],
        "Test Loss": loss_val,

        # Class 0 (Fake News) Stats
        "Fake (0) Precision": report['0']['precision'],
        "Fake (0) Recall": report['0']['recall'],
        "Fake (0) F1-Score": report['0']['f1-score'],

        # Class 1 (True News) Stats
        "True (1) Precision": report['1']['precision'],
        "True (1) Recall": report['1']['recall'],
        "True (1) F1-Score": report['1']['f1-score'],

        # Averages
        "Macro Avg Precision": report['macro avg']['precision'],
        "Macro Avg Recall": report['macro avg']['recall'],
        "Macro Avg F1": report['macro avg']['f1-score'],

        "Weighted Avg Precision": report['weighted avg']['precision'],
        "Weighted Avg Recall": report['weighted avg']['recall'],
        "Weighted Avg F1": report['weighted avg']['f1-score']
    }

    return metrics

# --- PROCESS MACHINE LEARNING MODELS ---
ml_models_map = {
    "Logistic Regression": LR, "Decision Tree": DTC, "Random Forest": RFC,
    "Gradient Boosting": GBC, "SVM (Linear)": SVM, "KNN": KNN, "Naive Bayes": NB,
    "Bagging (Decision Tree)": bagging_dt, "Bagging (Logistic Reg)": bagging_lr,
    "AdaBoost": ada, "XGBoost": xgb, "Stacking Classifier": stacking
}

for name, model in ml_models_map.items():
    try:
        results_list.append(get_full_metrics(name, "Machine Learning", model, is_dl=False))
    except NameError:
        pass

# --- PROCESS DEEP LEARNING MODELS ---
for name, model in dl_models.items():
    results_list.append(get_full_metrics(name.upper(), "Deep Learning", model, is_dl=True))

# --- CREATE DATAFRAME ---
df_full = pd.DataFrame(results_list).round(4)
df_full = df_full.sort_values(by="Accuracy", ascending=False)

# --- EXPORT TO EXCEL ---
filename = "Full_Detailed_Report.xlsx"

with pd.ExcelWriter(filename, engine='xlsxwriter') as writer:
    # We save the BIG table
    df_full.to_excel(writer, sheet_name='All Metrics', index=False)

    # Formatting
    workbook = writer.book
    worksheet = writer.sheets['All Metrics']

    # Add color to headers
    header_fmt = workbook.add_format({'bold': True, 'fg_color': '#D7E4BC', 'border': 1})
    for col_num, value in enumerate(df_full.columns.values):
        worksheet.write(0, col_num, value, header_fmt)

    # Adjust widths
    worksheet.set_column('A:B', 25)  # Name columns
    worksheet.set_column('C:O', 12)  # Number columns

print(f"Report generated: {filename}")

try:
    from google.colab import files
    files.download(filename)
except ImportError:
    print(f"File saved locally as {filename}")